# Copyright 2022-2023 XProbe Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import base64
import logging
import os
from io import BytesIO
from typing import Generator, List, Optional

import gradio as gr
import PIL.Image
from gradio.components import Markdown, Textbox
from gradio.layouts import Accordion, Column, Row

from ..client.restful.restful_client import (
    RESTfulChatModelHandle,
    RESTfulGenerateModelHandle,
)
from ..types import ChatCompletionMessage

logger = logging.getLogger(__name__)


class GradioInterface:
    """
    GradioInterface ç±»ç”¨äºæ„å»ºåŸºäº Gradio çš„èŠå¤©ç•Œé¢ã€‚
    
    è¯¥ç±»è´Ÿè´£åˆ›å»ºå’Œé…ç½®ä¸åŒç±»å‹çš„èŠå¤©ç•Œé¢ï¼ŒåŒ…æ‹¬æ™®é€šèŠå¤©ã€è§†è§‰è¯­è¨€èŠå¤©ç­‰ã€‚
    å®ƒæ ¹æ®æ¨¡å‹çš„èƒ½åŠ›å’Œç‰¹æ€§æ¥å†³å®šæ„å»ºä½•ç§ç±»å‹çš„ç•Œé¢ã€‚

    å±æ€§:
        endpoint (str): API ç«¯ç‚¹ URL
        model_uid (str): æ¨¡å‹çš„å”¯ä¸€æ ‡è¯†ç¬¦
        model_name (str): æ¨¡å‹åç§°
        model_size_in_billions (int): æ¨¡å‹å‚æ•°é‡ï¼ˆä»¥åäº¿ä¸ºå•ä½ï¼‰
        model_type (str): æ¨¡å‹ç±»å‹
        model_format (str): æ¨¡å‹æ ¼å¼
        quantization (str): é‡åŒ–æ–¹æ³•
        context_length (int): ä¸Šä¸‹æ–‡é•¿åº¦
        model_ability (List[str]): æ¨¡å‹èƒ½åŠ›åˆ—è¡¨
        model_description (str): æ¨¡å‹æè¿°
        model_lang (List[str]): æ¨¡å‹æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        _access_token (Optional[str]): è®¿é—®ä»¤ç‰Œï¼Œç”¨äº API è®¤è¯
    """

    def __init__(
        self,
        endpoint: str,
        model_uid: str,
        model_name: str,
        model_size_in_billions: int,
        model_type: str,
        model_format: str,
        quantization: str,
        context_length: int,
        model_ability: List[str],
        model_description: str,
        model_lang: List[str],
        access_token: Optional[str],
    ):
        """
        åˆå§‹åŒ– GradioInterface å®ä¾‹ã€‚

        å‚æ•°:
            endpoint (str): API ç«¯ç‚¹ URL
            model_uid (str): æ¨¡å‹çš„å”¯ä¸€æ ‡è¯†ç¬¦
            model_name (str): æ¨¡å‹åç§°
            model_size_in_billions (int): æ¨¡å‹å‚æ•°é‡ï¼ˆä»¥åäº¿ä¸ºå•ä½ï¼‰
            model_type (str): æ¨¡å‹ç±»å‹
            model_format (str): æ¨¡å‹æ ¼å¼
            quantization (str): é‡åŒ–æ–¹æ³•
            context_length (int): ä¸Šä¸‹æ–‡é•¿åº¦
            model_ability (List[str]): æ¨¡å‹èƒ½åŠ›åˆ—è¡¨
            model_description (str): æ¨¡å‹æè¿°
            model_lang (List[str]): æ¨¡å‹æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
            access_token (Optional[str]): è®¿é—®ä»¤ç‰Œï¼Œç”¨äº API è®¤è¯
        """
        self.endpoint = endpoint
        self.model_uid = model_uid
        self.model_name = model_name
        self.model_size_in_billions = model_size_in_billions
        self.model_type = model_type
        self.model_format = model_format
        self.quantization = quantization
        self.context_length = context_length
        self.model_ability = model_ability
        self.model_description = model_description
        self.model_lang = model_lang
        self._access_token = (
            access_token.replace("Bearer ", "") if access_token is not None else None
        )

    def build(self) -> "gr.Blocks":
        """
        æ„å»º Gradio ç•Œé¢ã€‚

        æ ¹æ®æ¨¡å‹èƒ½åŠ›é€‰æ‹©åˆé€‚çš„ç•Œé¢ç±»å‹ï¼Œå¹¶é…ç½®ç•Œé¢å±æ€§ã€‚

        è¿”å›:
            gr.Blocks: é…ç½®å¥½çš„ Gradio ç•Œé¢å¯¹è±¡
        """
        if "vision" in self.model_ability:
            interface = self.build_chat_vl_interface()
        elif "chat" in self.model_ability:
            interface = self.build_chat_interface()
        else:
            interface = self.build_generate_interface()

        interface.queue()
        # Gradio initiates the queue during a startup event, but since the app has already been
        # started, that event will not run, so manually invoke the startup events.
        # See: https://github.com/gradio-app/gradio/issues/5228
        interface.startup_events()
        # è®¾ç½®ç½‘é¡µå›¾æ ‡
        favicon_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)),
            os.path.pardir,
            "web",
            "ui",
            "public",
            "favicon.svg",
        )
        interface.favicon_path = favicon_path
        return interface

    def build_chat_interface(
        self,
    ) -> "gr.Blocks":
        """
        æ„å»ºæ™®é€šèŠå¤©ç•Œé¢ã€‚

        è¿”å›:
            gr.Blocks: é…ç½®å¥½çš„èŠå¤©ç•Œé¢å¯¹è±¡
        """
        def flatten(matrix: List[List[str]]) -> List[str]:
            """
            å°†äºŒç»´åˆ—è¡¨æ‰å¹³åŒ–ä¸ºä¸€ç»´åˆ—è¡¨ã€‚

            å‚æ•°:
                matrix (List[List[str]]): äºŒç»´å­—ç¬¦ä¸²åˆ—è¡¨

            è¿”å›:
                List[str]: æ‰å¹³åŒ–åçš„ä¸€ç»´åˆ—è¡¨
            """
            flat_list = []
            for row in matrix:
                flat_list += row
            return flat_list

        def to_chat(lst: List[str]) -> List[ChatCompletionMessage]:
            """
            å°†å­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢ä¸ºèŠå¤©å®Œæˆæ¶ˆæ¯åˆ—è¡¨ã€‚

            å‚æ•°:
                lst (List[str]): å­—ç¬¦ä¸²åˆ—è¡¨

            è¿”å›:
                List[ChatCompletionMessage]: èŠå¤©å®Œæˆæ¶ˆæ¯åˆ—è¡¨
            """
            res = []
            for i in range(len(lst)):
                role = "assistant" if i % 2 == 1 else "user"
                res.append(ChatCompletionMessage(role=role, content=lst[i]))
            return res

        def generate_wrapper(
            message: str,
            history: List[List[str]],
            max_tokens: int,
            temperature: float,
            lora_name: str,
        ) -> Generator:
            """
            èŠå¤©ç”Ÿæˆå‡½æ•°çš„åŒ…è£…å™¨ã€‚

            å‚æ•°:
                message (str): ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
                history (List[List[str]]): èŠå¤©å†å²
                max_tokens (int): ç”Ÿæˆçš„æœ€å¤§æ ‡è®°æ•°
                temperature (float): ç”Ÿæˆçš„æ¸©åº¦å‚æ•°
                lora_name (str): LoRA æ¨¡å‹åç§°

            è¿”å›:
                Generator: ç”Ÿæˆçš„å“åº”å†…å®¹ç”Ÿæˆå™¨
            """
            from ..client import RESTfulClient

            client = RESTfulClient(self.endpoint)
            client._set_token(self._access_token)
            model = client.get_model(self.model_uid)
            assert isinstance(model, RESTfulChatModelHandle)

            response_content = ""
            for chunk in model.chat(
                prompt=message,
                chat_history=to_chat(flatten(history)),
                generate_config={
                    "max_tokens": int(max_tokens),
                    "temperature": temperature,
                    "stream": True,
                    "lora_name": lora_name,
                },
            ):
                assert isinstance(chunk, dict)
                delta = chunk["choices"][0]["delta"]
                if "content" not in delta:
                    continue
                else:
                    response_content += delta["content"]
                    yield response_content

            yield response_content

        return gr.ChatInterface(
            fn=generate_wrapper,
            additional_inputs=[
                gr.Slider(
                    minimum=1,
                    maximum=self.context_length,
                    value=512,
                    step=1,
                    label="Max Tokens",
                ),
                gr.Slider(
                    minimum=0, maximum=2, value=1, step=0.01, label="Temperature"
                ),
                gr.Text(label="LoRA Name"),
            ],
            title=f"ğŸš€ Xinference Chat Bot : {self.model_name} ğŸš€",
            css="""
            .center{
                display: flex;
                justify-content: center;
                align-items: center;
                padding: 0px;
                color: #9ea4b0 !important;
            }
            """,
            description=f"""
            <div class="center">
            Model ID: {self.model_uid}
            </div>
            <div class="center">
            Model Size: {self.model_size_in_billions} Billion Parameters
            </div>
            <div class="center">
            Model Format: {self.model_format}
            </div>
            <div class="center">
            Model Quantization: {self.quantization}
            </div>
            """,
            analytics_enabled=False,
        )

    def build_chat_vl_interface(
        self,
    ) -> "gr.Blocks":
        """
        æ„å»ºè§†è§‰è¯­è¨€èŠå¤©ç•Œé¢ã€‚

        è¿”å›:
            gr.Blocks: é…ç½®å¥½çš„è§†è§‰è¯­è¨€èŠå¤©ç•Œé¢å¯¹è±¡
        """
        def predict(history, bot, max_tokens, temperature, stream):
            """
            é¢„æµ‹å‡½æ•°ï¼Œç”¨äºç”ŸæˆèŠå¤©å“åº”ã€‚

            å‚æ•°:
                history (List): èŠå¤©å†å²
                bot (List): æœºå™¨äººå“åº”åˆ—è¡¨
                max_tokens (int): ç”Ÿæˆçš„æœ€å¤§æ ‡è®°æ•°
                temperature (float): ç”Ÿæˆçš„æ¸©åº¦å‚æ•°
                stream (bool): æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º

            è¿”å›:
                Generator: ç”Ÿæˆçš„å†å²å’Œæœºå™¨äººå“åº”
            """
            from ..client import RESTfulClient

            client = RESTfulClient(self.endpoint)
            client._set_token(self._access_token)
            model = client.get_model(self.model_uid)
            assert isinstance(model, RESTfulChatModelHandle)

            prompt = history[-1]
            assert prompt["role"] == "user"
            prompt = prompt["content"]
            # å¤šæ¨¡æ€èŠå¤©ä¸æ”¯æŒæµå¼è¾“å‡º
            if stream:
                response_content = ""
                for chunk in model.chat(
                    prompt=prompt,
                    chat_history=history[:-1],
                    generate_config={
                        "max_tokens": max_tokens,
                        "temperature": temperature,
                        "stream": stream,
                    },
                ):
                    assert isinstance(chunk, dict)
                    delta = chunk["choices"][0]["delta"]
                    if "content" not in delta:
                        continue
                    else:
                        response_content += delta["content"]
                        bot[-1][1] = response_content
                        yield history, bot
                history.append(
                    {
                        "content": response_content,
                        "role": "assistant",
                    }
                )
                bot[-1][1] = response_content
                yield history, bot
            else:
                response = model.chat(
                    prompt=prompt,
                    chat_history=history[:-1],
                    generate_config={
                        "max_tokens": max_tokens,
                        "temperature": temperature,
                        "stream": stream,
                    },
                )
                history.append(response["choices"][0]["message"])
                bot[-1][1] = history[-1]["content"]
                yield history, bot

        def add_text(history, bot, text, image, video):
            """
            æ·»åŠ ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ã€å›¾ç‰‡æˆ–è§†é¢‘åˆ°èŠå¤©å†å²è®°å½•ä¸­ã€‚

            æ­¤å‡½æ•°å¤„ç†ç”¨æˆ·çš„è¾“å…¥ï¼ŒåŒ…æ‹¬çº¯æ–‡æœ¬ã€å›¾ç‰‡å’Œè§†é¢‘ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°èŠå¤©å†å²å’Œæœºå™¨äººå“åº”åˆ—è¡¨ä¸­ã€‚

            å‚æ•°:
            history (List): èŠå¤©å†å²è®°å½•åˆ—è¡¨
            bot (List): æœºå™¨äººå“åº”åˆ—è¡¨
            text (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            image (str): ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„
            video (str): ç”¨æˆ·ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶è·¯å¾„

            è¿”å›:
            Tuple[List, List, str, None, None]: æ›´æ–°åçš„å†å²è®°å½•ã€æœºå™¨äººå“åº”ã€æ¸…ç©ºçš„æ–‡æœ¬æ¡†ã€æ¸…ç©ºçš„å›¾ç‰‡å’Œè§†é¢‘è¾“å…¥
            """
            logger.debug("Add text, text: %s, image: %s, video: %s", text, image, video)
            
            if image:
                # å¤„ç†å›¾ç‰‡è¾“å…¥
                buffered = BytesIO()
                with PIL.Image.open(image) as img:
                    # è°ƒæ•´å›¾ç‰‡å¤§å°
                    img.thumbnail((500, 500))
                    img.save(buffered, format="JPEG")
                # å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64ç¼–ç 
                img_b64_str = base64.b64encode(buffered.getvalue()).decode()
                # å‡†å¤‡æ˜¾ç¤ºå†…å®¹å’Œæ¶ˆæ¯
                display_content = f'<img src="data:image/png;base64,{img_b64_str}" alt="user upload image" />\n{text}'
                message = {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": text},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{img_b64_str}"
                            },
                        },
                    ],
                }
            elif video:
                # å¤„ç†è§†é¢‘è¾“å…¥
                def video_to_base64(video_path):
                    """å°†è§†é¢‘æ–‡ä»¶è½¬æ¢ä¸ºbase64ç¼–ç """
                    with open(video_path, "rb") as video_file:
                        encoded_string = base64.b64encode(video_file.read()).decode(
                            "utf-8"
                        )
                    return encoded_string

                def generate_html_video(video_path):
                    """ç”ŸæˆåŒ…å«è§†é¢‘çš„HTMLä»£ç """
                    base64_video = video_to_base64(video_path)
                    video_format = video_path.split(".")[-1]
                    html_code = f"""
                    <video controls>
                        <source src="data:video/{video_format};base64,{base64_video}" type="video/{video_format}">
                        Your browser does not support the video tag.
                    </video>
                    """
                    return html_code

                # å‡†å¤‡æ˜¾ç¤ºå†…å®¹å’Œæ¶ˆæ¯
                display_content = f"{generate_html_video(video)}\n{text}"
                message = {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": text},
                        {
                            "type": "video_url",
                            "video_url": {"url": video},
                        },
                    ],
                }
            else:
                # å¤„ç†çº¯æ–‡æœ¬è¾“å…¥
                display_content = text
                message = {"role": "user", "content": text}
            
            # æ›´æ–°å†å²è®°å½•å’Œæœºå™¨äººå“åº”
            history = history + [message]
            bot = bot + [[display_content, None]]
            return history, bot, "", None, None

        def clear_history():
            """
            æ¸…ç©ºèŠå¤©å†å²è®°å½•ã€‚

            æ­¤å‡½æ•°ç”¨äºé‡ç½®èŠå¤©ç•Œé¢ï¼Œæ¸…é™¤æ‰€æœ‰å†å²è®°å½•å’Œè¾“å…¥ã€‚

            è¿”å›:
            Tuple[List, None, str, None, None]: ç©ºçš„å†å²è®°å½•ã€æ¸…ç©ºçš„æœºå™¨äººå“åº”ã€ç©ºæ–‡æœ¬æ¡†ã€æ¸…ç©ºçš„å›¾ç‰‡å’Œè§†é¢‘è¾“å…¥
            """
            logger.debug("Clear history.")
            return [], None, "", None, None

        def update_button(text):
            """
            æ›´æ–°å‘é€æŒ‰é’®çš„çŠ¶æ€ã€‚

            æ ¹æ®æ–‡æœ¬æ¡†æ˜¯å¦æœ‰å†…å®¹æ¥å¯ç”¨æˆ–ç¦ç”¨å‘é€æŒ‰é’®ã€‚

            å‚æ•°:
            text (str): æ–‡æœ¬æ¡†ä¸­çš„å†…å®¹

            è¿”å›:
            gr.update: Gradioæ›´æ–°å¯¹è±¡ï¼Œç”¨äºæ›´æ–°æŒ‰é’®çŠ¶æ€
            """
            return gr.update(interactive=bool(text))

        with gr.Blocks(
            title=f"ğŸš€ Xinference Chat Bot : {self.model_name} ğŸš€",
            css="""
        .center{
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 0px;
            color: #9ea4b0 !important;
        }
        """,
            analytics_enabled=False,
        ) as chat_vl_interface:
            # åˆ›å»ºèŠå¤©ç•Œé¢çš„æ ‡é¢˜å’Œæ¨¡å‹ä¿¡æ¯
            Markdown(
                f"""
                <h1 style='text-align: center; margin-bottom: 1rem'>ğŸš€ Xinference Chat Bot : {self.model_name} ğŸš€</h1>
                """
            )
            Markdown(
                f"""
                <div class="center">
                Model ID: {self.model_uid}
                </div>
                <div class="center">
                Model Size: {self.model_size_in_billions} Billion Parameters
                </div>
                <div class="center">
                Model Format: {self.model_format}
                </div>
                <div class="center">
                Model Quantization: {self.quantization}
                </div>
                """
            )

            # åˆå§‹åŒ–èŠå¤©çŠ¶æ€
            state = gr.State([])
            with gr.Row():
                # åˆ›å»ºèŠå¤©æœºå™¨äººç•Œé¢
                chatbot = gr.Chatbot(
                    elem_id="chatbot", label=self.model_name, height=700, scale=7
                )
                with gr.Column(scale=3):
                    # åˆ›å»ºå›¾ç‰‡ã€è§†é¢‘å’Œæ–‡æœ¬è¾“å…¥æ¡†
                    imagebox = gr.Image(type="filepath")
                    videobox = gr.Video()
                    textbox = gr.Textbox(
                        show_label=False,
                        placeholder="Enter text and press ENTER",
                        container=False,
                    )
                    # åˆ›å»ºå‘é€å’Œæ¸…é™¤æŒ‰é’®
                    submit_btn = gr.Button(
                        value="Send", variant="primary", interactive=False
                    )
                    clear_btn = gr.Button(value="Clear")

            # åˆ›å»ºé¢å¤–è¾“å…¥é€‰é¡¹ï¼ˆæŠ˜å é¢æ¿ï¼‰
            with gr.Accordion("Additional Inputs", open=False):
                max_tokens = gr.Slider(
                    minimum=1,
                    maximum=self.context_length,
                    value=512,
                    step=1,
                    label="Max Tokens",
                )
                temperature = gr.Slider(
                    minimum=0, maximum=2, value=1, step=0.01, label="Temperature"
                )
                stream = gr.Checkbox(label="Stream", value=False)

            # è®¾ç½®æ–‡æœ¬æ¡†å˜åŒ–æ—¶æ›´æ–°æŒ‰é’®çŠ¶æ€
            textbox.change(update_button, [textbox], [submit_btn], queue=False)

            # è®¾ç½®æ–‡æœ¬æ¡†æäº¤äº‹ä»¶
            textbox.submit(
                add_text,
                [state, chatbot, textbox, imagebox, videobox],
                [state, chatbot, textbox, imagebox, videobox],
                queue=False,
            ).then(
                predict,
                [state, chatbot, max_tokens, temperature, stream],
                [state, chatbot],
            )

            # è®¾ç½®å‘é€æŒ‰é’®ç‚¹å‡»äº‹ä»¶
            submit_btn.click(
                add_text,
                [state, chatbot, textbox, imagebox, videobox],
                [state, chatbot, textbox, imagebox, videobox],
                queue=False,
            ).then(
                predict,
                [state, chatbot, max_tokens, temperature, stream],
                [state, chatbot],
            )

            clear_btn.click(
                clear_history,
                None,
                [state, chatbot, textbox, imagebox, videobox],
                queue=False,
            )

        return chat_vl_interface

    def build_generate_interface(
        self,
    ):
        """
        æ„å»ºç”Ÿæˆå¼æ¨¡å‹çš„äº¤äº’ç•Œé¢ã€‚

        æ­¤æ–¹æ³•åˆ›å»ºä¸€ä¸ªç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ Gradio ç•Œé¢ï¼ŒåŒ…æ‹¬æ–‡æœ¬è¾“å…¥ã€ç”Ÿæˆæ§åˆ¶å’Œå†å²è®°å½•ç®¡ç†ã€‚

        è¿”å›:
            gr.Blocks: é…ç½®å¥½çš„ Gradio ç•Œé¢å¯¹è±¡

        ä¸»è¦åŠŸèƒ½:
        1. åˆ›å»ºæ–‡æœ¬è¾“å…¥å’Œè¾“å‡ºåŒºåŸŸ
        2. æä¾›ç”Ÿæˆã€æ’¤é”€ã€é‡è¯•å’Œæ¸…é™¤ç­‰æ“ä½œæŒ‰é’®
        3. å…è®¸ç”¨æˆ·è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼ˆå¦‚æœ€å¤§æ ‡è®°æ•°å’Œæ¸©åº¦ï¼‰
        4. ç®¡ç†ç”Ÿæˆå†å²è®°å½•
        """

        def undo(text, hist):
            """
            æ’¤é”€ä¸Šä¸€æ¬¡æ“ä½œï¼Œæ¢å¤åˆ°å‰ä¸€ä¸ªçŠ¶æ€ã€‚

            å‚æ•°:
                text (str): å½“å‰æ–‡æœ¬æ¡†ä¸­çš„å†…å®¹
                hist (list): å†å²è®°å½•åˆ—è¡¨

            è¿”å›:
                dict: åŒ…å«æ›´æ–°åçš„æ–‡æœ¬æ¡†å†…å®¹å’Œå†å²è®°å½•çš„å­—å…¸
            """
            if len(hist) == 0:
                return {
                    textbox: "",
                    history: [text],
                }
            if text == hist[-1]:
                hist = hist[:-1]

            return {
                textbox: hist[-1] if len(hist) > 0 else "",
                history: hist,
            }

        def clear(text, hist):
            """
            æ¸…é™¤å½“å‰æ–‡æœ¬å¹¶æ›´æ–°å†å²è®°å½•ã€‚

            å‚æ•°:
                text (str): å½“å‰æ–‡æœ¬æ¡†ä¸­çš„å†…å®¹
                hist (list): å†å²è®°å½•åˆ—è¡¨

            è¿”å›:
                dict: åŒ…å«æ¸…ç©ºåçš„æ–‡æœ¬æ¡†å†…å®¹å’Œæ›´æ–°åçš„å†å²è®°å½•çš„å­—å…¸
            """
            if len(hist) == 0 or (len(hist) > 0 and text != hist[-1]):
                hist.append(text)
            hist.append("")
            return {
                textbox: "",
                history: hist,
            }

        def complete(text, hist, max_tokens, temperature, lora_name) -> Generator:
            """
            ç”Ÿæˆæ–‡æœ¬å¹¶æ›´æ–°ç•Œé¢ã€‚

            å‚æ•°:
                text (str): å½“å‰æ–‡æœ¬æ¡†ä¸­çš„å†…å®¹
                hist (list): å†å²è®°å½•åˆ—è¡¨
                max_tokens (int): ç”Ÿæˆçš„æœ€å¤§æ ‡è®°æ•°
                temperature (float): ç”Ÿæˆçš„æ¸©åº¦å‚æ•°
                lora_name (str): LoRA æ¨¡å‹åç§°

            è¿”å›:
                Generator: ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹å’Œæ›´æ–°åçš„å†å²è®°å½•
            """
            from ..client import RESTfulClient

            # åˆå§‹åŒ–å®¢æˆ·ç«¯å¹¶è·å–æ¨¡å‹
            client = RESTfulClient(self.endpoint)
            client._set_token(self._access_token)
            model = client.get_model(self.model_uid)
            assert isinstance(model, RESTfulGenerateModelHandle)

            # æ›´æ–°å†å²è®°å½•
            if len(hist) == 0 or (len(hist) > 0 and text != hist[-1]):
                hist.append(text)

            response_content = text
            # ä½¿ç”¨æµå¼ç”Ÿæˆæ–‡æœ¬
            for chunk in model.generate(
                prompt=text,
                generate_config={
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "stream": True,
                    "lora_name": lora_name,
                },
            ):
                assert isinstance(chunk, dict)
                choice = chunk["choices"][0]
                if "text" not in choice:
                    continue
                else:
                    response_content += choice["text"]
                    yield {
                        textbox: response_content,
                        history: hist,
                    }

            # æ›´æ–°å†å²è®°å½•å¹¶è¿”å›æœ€ç»ˆç»“æœ
            hist.append(response_content)
            return {  # type: ignore
                textbox: response_content,
                history: hist,
            }

        def retry(text, hist, max_tokens, temperature, lora_name) -> Generator:
            """
            é‡æ–°ç”Ÿæˆæ–‡æœ¬ï¼Œä½¿ç”¨å†å²è®°å½•ä¸­çš„å‰ä¸€ä¸ªè¾“å…¥ã€‚

            å‚æ•°:
                text (str): å½“å‰æ–‡æœ¬æ¡†ä¸­çš„å†…å®¹
                hist (list): å†å²è®°å½•åˆ—è¡¨
                max_tokens (int): ç”Ÿæˆçš„æœ€å¤§æ ‡è®°æ•°
                temperature (float): ç”Ÿæˆçš„æ¸©åº¦å‚æ•°
                lora_name (str): LoRA æ¨¡å‹åç§°

            è¿”å›:
                Generator: é‡æ–°ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹å’Œæ›´æ–°åçš„å†å²è®°å½•
            """
            from ..client import RESTfulClient

            # åˆå§‹åŒ–å®¢æˆ·ç«¯å¹¶è·å–æ¨¡å‹
            client = RESTfulClient(self.endpoint)
            client._set_token(self._access_token)
            model = client.get_model(self.model_uid)
            assert isinstance(model, RESTfulGenerateModelHandle)

            # æ›´æ–°å†å²è®°å½•å¹¶è·å–å‰ä¸€ä¸ªè¾“å…¥
            if len(hist) == 0 or (len(hist) > 0 and text != hist[-1]):
                hist.append(text)
            text = hist[-2] if len(hist) > 1 else ""

            response_content = text
            # ä½¿ç”¨æµå¼ç”Ÿæˆæ–‡æœ¬
            for chunk in model.generate(
                prompt=text,
                generate_config={
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "stream": True,
                    "lora_name": lora_name,
                },
            ):
                assert isinstance(chunk, dict)
                choice = chunk["choices"][0]
                if "text" not in choice:
                    continue
                else:
                    response_content += choice["text"]
                    yield {
                        textbox: response_content,
                        history: hist,
                    }

            # æ›´æ–°å†å²è®°å½•å¹¶è¿”å›æœ€ç»ˆç»“æœ
            hist.append(response_content)
            return {  # type: ignore
                textbox: response_content,
                history: hist,
            }

        # åˆ›å»º Gradio ç•Œé¢
        with gr.Blocks(
            title=f"ğŸš€ Xinference Generate Bot : {self.model_name} ğŸš€",
            css="""
            .center{
                display: flex;
                justify-content: center;
                align-items: center;
                padding: 0px;
                color: #9ea4b0 !important;
            }
            """,
            analytics_enabled=False,
        ) as generate_interface:
            # åˆå§‹åŒ–å†å²è®°å½•çŠ¶æ€
            history = gr.State([])

            # æ·»åŠ æ ‡é¢˜å’Œæ¨¡å‹ä¿¡æ¯
            Markdown(
                f"""
                <h1 style='text-align: center; margin-bottom: 1rem'>ğŸš€ Xinference Generate Bot : {self.model_name} ğŸš€</h1>
                """
            )
            Markdown(
                f"""
                <div class="center">
                Model ID: {self.model_uid}
                </div>
                <div class="center">
                Model Size: {self.model_size_in_billions} Billion Parameters
                </div>
                <div class="center">
                Model Format: {self.model_format}
                </div>
                <div class="center">
                Model Quantization: {self.quantization}
                </div>
                """
            )

            # åˆ›å»ºä¸»è¦ç•Œé¢å…ƒç´ 
            with Column(variant="panel"):
                # æ–‡æœ¬è¾“å…¥æ¡†
                textbox = Textbox(
                    container=False,
                    show_label=False,
                    label="Message",
                    placeholder="Type a message...",
                    lines=21,
                    max_lines=50,
                )

                # æ“ä½œæŒ‰é’®
                with Row():
                    btn_generate = gr.Button("Generate", variant="primary")
                with Row():
                    btn_undo = gr.Button("â†©ï¸  Undo")
                    btn_retry = gr.Button("ğŸ”„  Retry")
                    btn_clear = gr.Button("ğŸ—‘ï¸  Clear")

                # é™„åŠ è¾“å…¥é€‰é¡¹ï¼ˆæŠ˜å é¢æ¿ï¼‰
                with Accordion("Additional Inputs", open=False):
                    length = gr.Slider(
                        minimum=1,
                        maximum=self.context_length,
                        value=1024,
                        step=1,
                        label="Max Tokens",
                    )
                    temperature = gr.Slider(
                        minimum=0, maximum=2, value=1, step=0.01, label="Temperature"
                    )
                    lora_name = gr.Text(label="LoRA Name")

                # è®¾ç½®æŒ‰é’®ç‚¹å‡»äº‹ä»¶
                btn_generate.click(
                    fn=complete,
                    inputs=[textbox, history, length, temperature, lora_name],
                    outputs=[textbox, history],
                )

                btn_undo.click(
                    fn=undo,
                    inputs=[textbox, history],
                    outputs=[textbox, history],
                )

                btn_retry.click(
                    fn=retry,
                    inputs=[textbox, history, length, temperature, lora_name],
                    outputs=[textbox, history],
                )

                btn_clear.click(
                    fn=clear,
                    inputs=[textbox, history],
                    outputs=[textbox, history],
                )

        return generate_interface
